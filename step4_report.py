import concurrent.futures
import json
from datetime import datetime, date, timezone
import config
import random
from ai_helper import call_ai_cli
from models import ArticleAnalysis, ArticleTags, DailyInsight
from database_manager import get_db

def analyze_single_article(article_row):
    """
    Ph√¢n t√≠ch 1 b√†i b√°o.
    Input: article_row (tuple): (url, title, content, published_date)
    Output: ArticleAnalysis object
    """
    url, title, content, published_date = article_row
    
    # C·∫Øt ng·∫Øn n·ªôi dung n·∫øu qu√° d√†i
    content_snippet = content[:config.MAX_CHARS_PER_ARTICLE]
    
    prompt = f"""
    Ph√¢n t√≠ch b√†i b√°o t√†i ch√≠nh sau v√† tr√≠ch xu·∫•t th√¥ng tin d∆∞·ªõi d·∫°ng JSON.
    
    B√†i b√°o: {title}
    N·ªôi dung: {content_snippet}
    
    Y√™u c·∫ßu Output JSON ƒë√∫ng ƒë·ªãnh d·∫°ng sau (kh√¥ng markdown):
    {{
        "summary": "T√≥m t·∫Øt 3 c√¢u, t·∫≠p trung v√†o s·ªë li·ªáu v√† s·ª± ki·ªán",
        "language": "vi ho·∫∑c en",
        "importance_score": 1-10,
        "origin": "VN ho·∫∑c Global",
        "tags": {{
            "source": "Ngu·ªìn b√°o",
            "sectors": ["B·∫•t ƒë·ªông s·∫£n", "Ng√¢n h√†ng", ...],
            "entities": ["Vingroup", "Techcombank", ...],
            "people": ["Ph·∫°m Nh·∫≠t V∆∞·ª£ng", ...],
            "locations": ["TP.HCM", "H√† N·ªôi"],
            "keywords": ["FED", "L√£i su·∫•t", ...],
            "sentiment": "T√≠ch c·ª±c/Ti√™u c·ª±c/Trung l·∫≠p"
        }},
        "author_intent": "M·ª•c ƒë√≠ch b√†i vi·∫øt (PR, Tin t·ª©c, C·∫£nh b√°o, ...)",
        "impact_analysis": "D·ª± ƒëo√°n t√°c ƒë·ªông ng·∫Øn h·∫°n (TƒÉng/Gi·∫£m/·ªîn ƒë·ªãnh) ƒë·∫øn th·ªã tr∆∞·ªùng li√™n quan."
    }}
    """
    
    try:
        response = call_ai_cli(prompt, model=config.GEMINI_MODEL)
        cleaned = response.replace("```json", "").replace("```", "").strip()
        data = json.loads(cleaned)
        
        # Validate & Map to Pydantic Model
        analysis = ArticleAnalysis(
            url=url,
            title=title,
            summary=data.get("summary", ""),
            language=data.get("language", "vi"),
            importance_score=data.get("importance_score", 5),
            origin=data.get("origin", "VN"),
            tags=ArticleTags(**data.get("tags", {})),
            author_intent=data.get("author_intent"),
            impact_analysis=data.get("impact_analysis"),
            analyzed_at=datetime.now(),
            model_version=config.GEMINI_MODEL
        )
        return analysis
        
    except Exception as e:
        print(f"‚ùå Error analyzing {url}: {e}")
        return None

def generate_daily_insights(analyzed_articles):
    """
    T·ªïng h·ª£p insight t·ª´ danh s√°ch c√°c b√†i b√°o ƒë√£ ph√¢n t√≠ch trong ng√†y.
    """
    if not analyzed_articles:
        return None

    # Gom n·ªôi dung ƒë·ªÉ g·ª≠i cho AI t·ªïng h·ª£p
    articles_text = ""
    for idx, art in enumerate(analyzed_articles):
        articles_text += f"[{idx+1}] {art.title} (Sentiment: {art.tags.sentiment})\n"
        articles_text += f"   Summary: {art.summary}\n"
        articles_text += f"   Impact: {art.impact_analysis}\n\n"

    prompt = f"""
    D·ª±a tr√™n {len(analyzed_articles)} b√†i b√°o t√†i ch√≠nh sau ƒë√¢y, h√£y t·ªïng h·ª£p th√†nh B√°o C√°o Chi·∫øn L∆∞·ª£c Ng√†y.
    
    Danh s√°ch b√†i b√°o:
    {articles_text}
    
    Y√™u c·∫ßu Output JSON (kh√¥ng markdown):
    {{
        "date": "{datetime.now(timezone.utc).date()}",
        "main_trends": ["Xu h∆∞·ªõng ch√≠nh 1", "Xu h∆∞·ªõng ch√≠nh 2"],
        "hidden_insights": ["Insight kh√¥ng hi·ªÉn nhi√™n m√† b·∫°n nh·∫≠n ra t·ª´ d·ªØ li·ªáu tr√™n"],
        "media_steering_analysis": "Ph√¢n t√≠ch xem truy·ªÅn th√¥ng ƒëang mu·ªën l√°i d∆∞ lu·∫≠n theo h∆∞·ªõng n√†o (FUD, FOMO, hay Th·∫≠n tr·ªçng).",
        "hot_topics": ["Ch·ªß ƒë·ªÅ 1", "Ch·ªß ƒë·ªÅ 2"],
        "market_sentiment_overlay": "Nh·∫≠n ƒë·ªãnh chung v·ªÅ t√¢m l√Ω th·ªã tr∆∞·ªùng (Bullish/Bearish/Neutral) v√† l√Ω do."
    }}
    """
    
    try:
        response = call_ai_cli(prompt, model=config.GEMINI_MODEL)
        cleaned = response.replace("```json", "").replace("```", "").strip()
        data = json.loads(cleaned)
        
        return DailyInsight(**data)
    except Exception as e:
        print(f"‚ùå Error generating insights: {e}")
        return None

def generate_report():
    print("\n--- [Step 4] Analyzing & Reporting ---")
    
    all_processed_analyses = []
    
    with get_db() as conn:
        with conn.cursor() as cur:
            # 1. Get articles available for analysis
            cur.execute("""
                SELECT url, title, content, published_date 
                FROM articles 
                WHERE status = 'scraped' 
                AND published_date >= (NOW() AT TIME ZONE 'UTC') - INTERVAL '24 hours'
            """)
            rows = cur.fetchall()
            
            # GI·ªöI H·∫†N TRONG TEST MODE
            if config.TEST_MODE:
                if config.TEST_RANDOM:
                    print(f"üõ†Ô∏è [TEST MODE] L·∫•y ng·∫´u nhi√™n {config.TEST_LIMIT} b√†i ƒë·ªÉ ph√¢n t√≠ch.")
                    random.shuffle(rows)
                else:
                    print(f"üõ†Ô∏è [TEST MODE] L·∫•y {config.TEST_LIMIT} b√†i m·ªõi nh·∫•t ƒë·ªÉ ph√¢n t√≠ch.")
                rows = rows[:config.TEST_LIMIT]
            
            if not rows:
                print("‚ö†Ô∏è Kh√¥ng c√≥ b√†i b√°o n√†o c·∫ßn ph√¢n t√≠ch (status='scraped').")
                return None, 0
 
            total_articles = len(rows)
            print(f"üîç B·∫Øt ƒë·∫ßu ph√¢n t√≠ch {total_articles} b√†i b√°o (6 Hybrid workers, update immediately)...")
            
            # 2. Analyze & Save sequentially as tasks complete
            with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:
                # Submit all tasks
                future_to_url = {executor.submit(analyze_single_article, row): row[0] for row in rows}
                
                count = 0
                for future in concurrent.futures.as_completed(future_to_url):
                    url = future_to_url[future]
                    count += 1
                    try:
                        res = future.result()
                        if res:
                            all_processed_analyses.append(res)
                            # Update DB immediately for this article
                            cur.execute("""
                                UPDATE articles
                                SET summary = %s, tags = %s::jsonb, author_intent = %s, 
                                    impact_analysis = %s, analyzed_at = %s, model_version = %s, 
                                    language = %s, importance_score = %s, origin = %s, 
                                    status = 'analyzed'
                                WHERE url = %s
                            """, (
                                res.summary,
                                res.tags.model_dump_json(),
                                res.author_intent,
                                res.impact_analysis,
                                res.analyzed_at, # res.analyzed_at is already UTC from get_now_utc()
                                res.model_version,
                                res.language,
                                res.importance_score,
                                res.origin,
                                res.url
                            ))
                            conn.commit() # Commit ngay l·∫≠p t·ª©c
                            print(f"  ‚úÖ [{count}/{total_articles}] Analyzed & Saved: {res.title[:50]}...")
                        else:
                            print(f"  ‚ö†Ô∏è [{count}/{total_articles}] Failed analysis for: {url}")
                            
                    except Exception as e:
                        print(f"  ‚ùå [{count}/{total_articles}] Unexpected error for {url}: {e}")
            
            print(f"üéâ Ho√†n t·∫•t ph√¢n t√≠ch {len(all_processed_analyses)}/{total_articles} b√†i.")

            # 4. Generate Daily Insights (from ALL articles analyzed in last 24h)
            
            if all_processed_analyses:
                print("üß† ƒêang t·ªïng h·ª£p Insight th·ªã tr∆∞·ªùng...")
                daily_insight = generate_daily_insights(all_processed_analyses)
                
                if daily_insight:
                    # Save Insight to DB
                    try:
                        cur.execute("""
                            INSERT INTO daily_insights (date, main_trends, hidden_insights, media_steering_analysis, hot_topics, market_sentiment_overlay, created_at)
                            VALUES (%s, %s::jsonb, %s::jsonb, %s, %s::jsonb, %s, NOW() AT TIME ZONE 'UTC')
                            ON CONFLICT (date) DO UPDATE SET
                                main_trends = EXCLUDED.main_trends,
                                hidden_insights = EXCLUDED.hidden_insights,
                                media_steering_analysis = EXCLUDED.media_steering_analysis,
                                hot_topics = EXCLUDED.hot_topics,
                                market_sentiment_overlay = EXCLUDED.market_sentiment_overlay,
                                created_at = NOW() AT TIME ZONE 'UTC';
                        """, (
                            daily_insight.date,
                            json.dumps(daily_insight.main_trends, ensure_ascii=False),
                            json.dumps(daily_insight.hidden_insights, ensure_ascii=False),
                            daily_insight.media_steering_analysis,
                            json.dumps(daily_insight.hot_topics, ensure_ascii=False),
                            daily_insight.market_sentiment_overlay
                        ))
                        conn.commit()
                        print("‚úÖ ƒê√£ l∆∞u Daily Insight v√†o Database.")
                        return daily_insight, len(all_processed_analyses)
                        
                    except Exception as e:
                        print(f"‚ùå DB Error saving insight: {e}")
            
            return None, len(all_processed_analyses)

if __name__ == "__main__":
    generate_report()