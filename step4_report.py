import json
import os
import datetime
import concurrent.futures
import config
from gemini_helper import call_gemini_cli
from models import ArticleAnalysis, ArticleTags, DailyInsight

def load_db(file_path):
    if os.path.exists(file_path):
        with open(file_path, "r", encoding="utf-8") as f:
            try:
                return json.load(f)
            except:
                return {}
    return {}

def save_db(file_path, data):
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def analyze_single_article(article):
    url = article['url']
    title = article['title']
    content = article['text']
    
    analysis_db = load_db(config.ANALYSIS_DB_FILE)
    if url in analysis_db:
        print(f"‚è© ƒê√£ c√≥ ph√¢n t√≠ch cho b√†i: {title}. Skip.")
        return analysis_db[url]

    print(f"üß† ƒêang ph√¢n t√≠ch chuy√™n s√¢u: {title}")
    
    prompt = f"""
    B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch d·ªØ li·ªáu v√† truy·ªÅn th√¥ng t√†i ch√≠nh.
    H√£y ph√¢n t√≠ch b√†i b√°o sau ƒë√¢y v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng JSON duy nh·∫•t, tu√¢n th·ªß ƒë√∫ng ƒë·ªãnh d·∫°ng y√™u c·∫ßu.

    N·ªôi dung b√†i b√°o:
    Ti√™u ƒë·ªÅ: {title}
    N·ªôi dung: {content[:config.MAX_CHARS_PER_ARTICLE]}

    Y√™u c·∫ßu JSON Output:
    {{
        "url": "{url}",
        "title": "{title}",
        "summary": "T√≥m t·∫Øt ng·∫Øn g·ªçn 1-2 c√¢u",
        "tags": {{
            "source": "T√™n b√°o/ngu·ªìn tin",
            "sectors": ["Ng√†nh ngh·ªÅ"],
            "entities": ["T√™n c√¥ng ty"],
            "people": ["T√™n ng∆∞·ªùi"],
            "locations": ["ƒê·ªãa danh"],
            "keywords": ["T·ª´ kh√≥a"],
            "sentiment": "T√≠ch c·ª±c | Ti√™u c·ª±c | Trung l·∫≠p | Kh√¥ng x√°c ƒë·ªãnh"
        }},
        "author_intent": "M·ª•c ƒë√≠ch b√†i vi·∫øt",
        "impact_analysis": "Ph√¢n t√≠ch t√°c ƒë·ªông"
    }}
    """

    response_text = call_gemini_cli(prompt, model=config.GEMINI_MODEL)
    if not response_text:
        return None

    try:
        cleaned_text = response_text.replace("```json", "").replace("```", "").strip()
        raw_json = json.loads(cleaned_text)
        
        # Validate b·∫±ng Pydantic
        # L∆∞u √Ω: AI c√≥ th·ªÉ kh√¥ng tr·∫£ v·ªÅ url/title trong json, ta c·∫ßn inject v√†o
        raw_json["url"] = url
        raw_json["title"] = title
        
        analysis_model = ArticleAnalysis(**raw_json)
        
        # Serialize th√†nh dict ƒë·ªÉ l∆∞u JSON
        analysis_dict = json.loads(analysis_model.model_dump_json())
        
        # L∆∞u v√†o DB (l∆∞u √Ω concurrent write)
        analysis_db = load_db(config.ANALYSIS_DB_FILE)
        analysis_db[url] = analysis_dict
        save_db(config.ANALYSIS_DB_FILE, analysis_db)
        
        return analysis_dict
    except Exception as e:
        print(f"‚ùå L·ªói validation/parse b√†i '{title}': {e}")
        return None

def process_articles_parallel(articles):
    print(f"\n--- B·∫Øt ƒë·∫ßu ph√¢n t√≠ch song song {len(articles)} b√†i b√°o ---")
    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        future_to_article = {executor.submit(analyze_single_article, art): art for art in articles}
        for future in concurrent.futures.as_completed(future_to_article):
            res = future.result()
            if res:
                results.append(res)
    return results

def generate_daily_insight():
    print(f"üìä ƒêang t·ªïng h·ª£p Insight 24h qua...")
    
    analysis_db = load_db(config.ANALYSIS_DB_FILE)
    now = datetime.datetime.now()
    yesterday = now - datetime.timedelta(hours=24)
    
    recent_analyses = []
    for url, data in analysis_db.items():
        try:
            analyzed_at = datetime.datetime.fromisoformat(data['analyzed_at'])
            if analyzed_at > yesterday:
                recent_analyses.append(data)
        except:
            continue
            
    if not recent_analyses:
        print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ph√¢n t√≠ch trong 24h qua ƒë·ªÉ t·∫°o insight.")
        return

    context = ""
    for idx, data in enumerate(recent_analyses, 1):
        # Data ƒë√£ ƒë∆∞·ª£c normalized, truy xu·∫•t an to√†n
        tags = data.get('tags', {})
        context += f"B√†i {idx}: {data['title']}. T√≥m t·∫Øt: {data['summary']}. Tags: {tags}\n"

    prompt = f"""
    D·ª±a tr√™n c√°c ph√¢n t√≠ch b√†i b√°o trong 24h qua sau ƒë√¢y:
    {context}

    H√£y th·ª±c hi·ªán ph√¢n t√≠ch t·ªïng qu√°t (Insight Report).
    Tr·∫£ v·ªÅ k·∫øt qu·∫£ JSON v·ªõi c√°c tr∆∞·ªùng:
    {{
        "date": "{now.strftime('%Y-%m-%d')}",
        "main_trends": ["Chanel 1", "Chanel 2"],
        "hidden_insights": ["Insight 1"],
        "media_steering_analysis": "Text analysis...",
        "hot_topics": ["Topic 1"],
        "market_sentiment_overlay": "Text..."
    }}
    """

    response_text = call_gemini_cli(prompt, model=config.GEMINI_MODEL)
    if not response_text:
        return

    try:
        cleaned_text = response_text.replace("```json", "").replace("```", "").strip()
        raw_json = json.loads(cleaned_text)
        
        # Validate b·∫±ng Pydantic
        insight_model = DailyInsight(**raw_json)
        
        insight_dict = json.loads(insight_model.model_dump_json())
        date_str = str(insight_model.date)
        
        insights_db = load_db(config.DAILY_INSIGHTS_FILE)
        insights_db[date_str] = insight_dict
        save_db(config.DAILY_INSIGHTS_FILE, insights_db)
        
        # T·∫°o report Markdown
        report_md = f"# DAILY FINANCIAL INSIGHTS - {date_str}\n\n"
        report_md += f"*(Created at: {insight_model.created_at.strftime('%H:%M %d/%m/%Y')})*\n\n"
        report_md += "## üìà Xu h∆∞·ªõng ch√≠nh\n" + "\n".join([f"- {i}" for i in insight_model.main_trends]) + "\n\n"
        report_md += "## üí° Hidden Insights\n" + "\n".join([f"- {i}" for i in insight_model.hidden_insights]) + "\n\n"
        report_md += "## üó£Ô∏è Media Steering Analysis\n" + (insight_model.media_steering_analysis or "N/A") + "\n\n"
        report_md += "## üî• Hot Topics\n" + ", ".join(insight_model.hot_topics) + "\n"
        
        report_filename = f"daily_report_{date_str}.md"
        with open(report_filename, "w", encoding="utf-8") as f:
            f.write(report_md)
            
        print(f"‚úÖ ƒê√£ t·∫°o b√°o c√°o Insight: {report_filename}")
        return insight_dict
    except Exception as e:
        print(f"‚ùå L·ªói parse Daily Insight: {e}")

def generate_report(input_file):
    # ƒê·ªçc d·ªØ li·ªáu t·ª´ Node 3 (Content ƒë√£ c√†o)
    if not os.path.exists(input_file):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file {input_file}")
        return

    with open(input_file, "r", encoding="utf-8") as f:
        articles = json.load(f)

    if not articles:
        return

    # B∆∞·ªõc 1: Ph√¢n t√≠ch t·ª´ng b√†i song song
    process_articles_parallel(articles)
    
    # B∆∞·ªõc 2: T·ªïng h·ª£p Insight 24h
    generate_daily_insight()

if __name__ == "__main__":
    generate_report(config.STEP3_FILE)