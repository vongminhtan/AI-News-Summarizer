import concurrent.futures
import json
from datetime import datetime, date
import config
from gemini_helper import call_gemini_cli
from models import ArticleAnalysis, ArticleTags, DailyInsight
from database_manager import get_db

def analyze_single_article(article_row):
    """
    Ph√¢n t√≠ch 1 b√†i b√°o.
    Input: article_row (tuple): (url, title, content, published_date)
    Output: ArticleAnalysis object
    """
    url, title, content, published_date = article_row
    
    # C·∫Øt ng·∫Øn n·ªôi dung n·∫øu qu√° d√†i
    content_snippet = content[:config.MAX_CHARS_PER_ARTICLE]
    
    prompt = f"""
    Ph√¢n t√≠ch b√†i b√°o t√†i ch√≠nh sau v√† tr√≠ch xu·∫•t th√¥ng tin d∆∞·ªõi d·∫°ng JSON.
    
    B√†i b√°o: {title}
    N·ªôi dung: {content_snippet}
    
    Y√™u c·∫ßu Output JSON ƒë√∫ng ƒë·ªãnh d·∫°ng sau (kh√¥ng markdown):
    {{
        "summary": "T√≥m t·∫Øt 3 c√¢u, t·∫≠p trung v√†o s·ªë li·ªáu v√† s·ª± ki·ªán",
        "language": "vi ho·∫∑c en",
        "importance_score": 1-10,
        "origin": "VN ho·∫∑c Global",
        "tags": {{
            "source": "Ngu·ªìn b√°o",
            "sectors": ["B·∫•t ƒë·ªông s·∫£n", "Ng√¢n h√†ng", ...],
            "entities": ["Vingroup", "Techcombank", ...],
            "people": ["Ph·∫°m Nh·∫≠t V∆∞·ª£ng", ...],
            "locations": ["TP.HCM", "H√† N·ªôi"],
            "keywords": ["FED", "L√£i su·∫•t", ...],
            "sentiment": "T√≠ch c·ª±c/Ti√™u c·ª±c/Trung l·∫≠p"
        }},
        "author_intent": "M·ª•c ƒë√≠ch b√†i vi·∫øt (PR, Tin t·ª©c, C·∫£nh b√°o, ...)",
        "impact_analysis": "D·ª± ƒëo√°n t√°c ƒë·ªông ng·∫Øn h·∫°n (TƒÉng/Gi·∫£m/·ªîn ƒë·ªãnh) ƒë·∫øn th·ªã tr∆∞·ªùng li√™n quan."
    }}
    """
    
    try:
        response = call_gemini_cli(prompt, model=config.GEMINI_MODEL)
        cleaned = response.replace("```json", "").replace("```", "").strip()
        data = json.loads(cleaned)
        
        # Validate & Map to Pydantic Model
        analysis = ArticleAnalysis(
            url=url,
            title=title,
            summary=data.get("summary", ""),
            language=data.get("language", "vi"),
            importance_score=data.get("importance_score", 5),
            origin=data.get("origin", "VN"),
            tags=ArticleTags(**data.get("tags", {})),
            author_intent=data.get("author_intent"),
            impact_analysis=data.get("impact_analysis"),
            analyzed_at=datetime.now(),
            model_version=config.GEMINI_MODEL
        )
        return analysis
        
    except Exception as e:
        print(f"‚ùå Error analyzing {url}: {e}")
        return None

def generate_daily_insights(analyzed_articles):
    """
    T·ªïng h·ª£p insight t·ª´ danh s√°ch c√°c b√†i b√°o ƒë√£ ph√¢n t√≠ch trong ng√†y.
    """
    if not analyzed_articles:
        return None

    # Gom n·ªôi dung ƒë·ªÉ g·ª≠i cho AI t·ªïng h·ª£p
    articles_text = ""
    for idx, art in enumerate(analyzed_articles):
        articles_text += f"[{idx+1}] {art.title} (Sentiment: {art.tags.sentiment})\n"
        articles_text += f"   Summary: {art.summary}\n"
        articles_text += f"   Impact: {art.impact_analysis}\n\n"

    prompt = f"""
    D·ª±a tr√™n {len(analyzed_articles)} b√†i b√°o t√†i ch√≠nh sau ƒë√¢y, h√£y t·ªïng h·ª£p th√†nh B√°o C√°o Chi·∫øn L∆∞·ª£c Ng√†y.
    
    Danh s√°ch b√†i b√°o:
    {articles_text}
    
    Y√™u c·∫ßu Output JSON (kh√¥ng markdown):
    {{
        "date": "{date.today()}",
        "main_trends": ["Xu h∆∞·ªõng ch√≠nh 1", "Xu h∆∞·ªõng ch√≠nh 2"],
        "hidden_insights": ["Insight kh√¥ng hi·ªÉn nhi√™n m√† b·∫°n nh·∫≠n ra t·ª´ d·ªØ li·ªáu tr√™n"],
        "media_steering_analysis": "Ph√¢n t√≠ch xem truy·ªÅn th√¥ng ƒëang mu·ªën l√°i d∆∞ lu·∫≠n theo h∆∞·ªõng n√†o (FUD, FOMO, hay Th·∫≠n tr·ªçng).",
        "hot_topics": ["Ch·ªß ƒë·ªÅ 1", "Ch·ªß ƒë·ªÅ 2"],
        "market_sentiment_overlay": "Nh·∫≠n ƒë·ªãnh chung v·ªÅ t√¢m l√Ω th·ªã tr∆∞·ªùng (Bullish/Bearish/Neutral) v√† l√Ω do."
    }}
    """
    
    try:
        response = call_gemini_cli(prompt, model=config.GEMINI_MODEL)
        cleaned = response.replace("```json", "").replace("```", "").strip()
        data = json.loads(cleaned)
        
        return DailyInsight(**data)
    except Exception as e:
        print(f"‚ùå Error generating insights: {e}")
        return None

def generate_report():
    print("\n--- [Step 4] Analyzing & Reporting ---")
    
    processed_analyses = []
    
    with get_db() as conn:
        with conn.cursor() as cur:
            # 1. Get articles available for analysis
            cur.execute("SELECT url, title, content, published_date FROM articles WHERE status = 'scraped'")
            rows = cur.fetchall() # [(url, title, content, date), ...]
            
            if not rows:
                print("‚ö†Ô∏è Kh√¥ng c√≥ b√†i b√°o n√†o c·∫ßn ph√¢n t√≠ch (status='scraped').")
                return

            print(f"üîç B·∫Øt ƒë·∫ßu ph√¢n t√≠ch {len(rows)} b√†i b√°o (Parallel)...")
            
            # 2. Analyze Parallel
            with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                results = list(executor.map(analyze_single_article, rows))
            
            # 3. Save Analysis Results to DB
            count_success = 0
            for res in results:
                if res:
                    processed_analyses.append(res)
                    try:
                        cur.execute("""
                            UPDATE articles
                            SET summary = %s, tags = %s::jsonb, author_intent = %s, 
                                impact_analysis = %s, analyzed_at = %s, model_version = %s, 
                                language = %s, importance_score = %s, origin = %s, 
                                status = 'analyzed'
                            WHERE url = %s
                        """, (
                            res.summary,
                            res.tags.model_dump_json(), # Pydantic to JSON string
                            res.author_intent,
                            res.impact_analysis,
                            res.analyzed_at,
                            res.model_version,
                            res.language,
                            res.importance_score,
                            res.origin,
                            res.url
                        ))
                        count_success += 1
                    except Exception as e:
                        print(f"‚ùå DB Error saving analysis for {res.url}: {e}")
            
            conn.commit()
            print(f"‚úÖ ƒê√£ ph√¢n t√≠ch v√† l∆∞u {count_success} b√†i.")

            # 4. Generate Daily Insights (from ALL articles analyzed in last 24h)
            
            if processed_analyses:
                print("üß† ƒêang t·ªïng h·ª£p Insight th·ªã tr∆∞·ªùng...")
                daily_insight = generate_daily_insights(processed_analyses)
                
                if daily_insight:
                    # Save Insight to DB
                    try:
                        cur.execute("""
                            INSERT INTO daily_insights (date, main_trends, hidden_insights, media_steering_analysis, hot_topics, market_sentiment_overlay, created_at)
                            VALUES (%s, %s::jsonb, %s::jsonb, %s, %s::jsonb, %s, NOW())
                            ON CONFLICT (date) DO UPDATE SET
                                main_trends = EXCLUDED.main_trends,
                                hidden_insights = EXCLUDED.hidden_insights,
                                media_steering_analysis = EXCLUDED.media_steering_analysis,
                                hot_topics = EXCLUDED.hot_topics,
                                market_sentiment_overlay = EXCLUDED.market_sentiment_overlay,
                                created_at = NOW();
                        """, (
                            daily_insight.date,
                            json.dumps(daily_insight.main_trends, ensure_ascii=False),
                            json.dumps(daily_insight.hidden_insights, ensure_ascii=False),
                            daily_insight.media_steering_analysis,
                            json.dumps(daily_insight.hot_topics, ensure_ascii=False),
                            daily_insight.market_sentiment_overlay
                        ))
                        conn.commit()
                        print("‚úÖ ƒê√£ l∆∞u Daily Insight v√†o Database.")
                        
                        # Generate Markdown Report
                        report_file = f"daily_report_{daily_insight.date}.md"
                        with open(report_file, "w", encoding="utf-8") as f:
                            f.write(f"# üìä B√°o C√°o Th·ªã Tr∆∞·ªùng Ng√†y {daily_insight.date}\n\n")
                            f.write(f"### üå°Ô∏è T√¢m L√Ω Th·ªã Tr∆∞·ªùng: {daily_insight.market_sentiment_overlay}\n\n")
                            f.write("## üî• Hot Topics\n")
                            for topic in daily_insight.hot_topics:
                                f.write(f"- {topic}\n")
                            f.write("\n## üëÅÔ∏è Hidden Insights\n")
                            for insight in daily_insight.hidden_insights:
                                f.write(f"- {insight}\n")
                            f.write("\n## üß≠ Ph√¢n T√≠ch ƒêi·ªÅu H∆∞·ªõng Truy·ªÅn Th√¥ng\n")
                            f.write(f"{daily_insight.media_steering_analysis}\n")
                        print(f"üìÑ ƒê√£ xu·∫•t b√°o c√°o Markdown: {report_file}")
                        
                    except Exception as e:
                        print(f"‚ùå DB Error saving insight: {e}")

if __name__ == "__main__":
    generate_report()