import signal
import time
import concurrent.futures
from newspaper import Article, Config
from database_manager import get_db
import config

# Timeout handler
def handler(signum, frame):
    raise TimeoutError("Scraping timed out")

# Register signal
signal.signal(signal.SIGALRM, handler)

def scrape_single_url(url):
    """
    HÃ m cÃ o dá»¯ liá»‡u cho 1 URL (Cháº¡y trong Thread).
    KhÃ´ng káº¿t ná»‘i DB á»Ÿ Ä‘Ã¢y Ä‘á»ƒ trÃ¡nh lá»—i Tunnel.
    """
    try:
        # Newspaper Config
        news_config = Config()
        news_config.browser_user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        news_config.request_timeout = 10
        
        article = Article(url, config=news_config)
        article.download()
        article.parse()
        
        content = article.text
        if not content or len(content) < 100:
            return (url, None, "Content too short")
            
        return (url, content, None)
        
    except Exception as e:
        return (url, None, str(e))

def scrape_articles():
    print("\n--- [Step 3] Scraping Content (Parallel) ---")
    
    with get_db() as conn:
        with conn.cursor() as cur:
            # 1. Get articles that passed the filter
            cur.execute("SELECT url FROM articles WHERE status = 'filtered_in'")
            rows = cur.fetchall()
            
            if not rows:
                print("âš ï¸ KhÃ´ng cÃ³ bÃ i bÃ¡o nÃ o cáº§n cÃ o (status='filtered_in').")
                return []

            target_urls = [r[0] for r in rows]
            print(f"ðŸš€ Báº¯t Ä‘áº§u cÃ o {len(target_urls)} bÃ i (5 threads)...")

            success_count = 0
            
            # 2. Run Parallel Scraping
            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                results = list(executor.map(scrape_single_url, target_urls))
            
            # 3. Update DB Sequentially
            for url, content, error in results:
                if content:
                    try:
                        cur.execute("""
                            UPDATE articles 
                            SET content = %s, scraped_at = NOW(), status = 'scraped'
                            WHERE url = %s
                        """, (content, url))
                        success_count += 1
                        print(f"âœ… Scraped: {url}")
                    except Exception as e:
                        print(f"âŒ DB Error {url}: {e}")
                else:
                    print(f"âš ï¸ Failed {url}: {error}")
                    # Optional: Update status to 'failed' or keep 'filtered_in' to retry later
            
            conn.commit()
            print(f"ðŸŽ‰ HoÃ n táº¥t cÃ o {success_count}/{len(target_urls)} bÃ i.")
            return target_urls

if __name__ == "__main__":
    scrape_articles()