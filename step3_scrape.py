import json
import os
import time
import config
from newspaper import Article, Config

def scrape_content(input_file):
    if not os.path.exists(input_file):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file {input_file}")
        return []

    with open(input_file, "r", encoding="utf-8") as f:
        links = json.load(f)

    scraped_data = []
    
    # C·∫•u h√¨nh t·ª´ file config
    art_config = Config()
    art_config.browser_user_agent = config.USER_AGENT
    art_config.request_timeout = config.SCRAPE_TIMEOUT

    print(f"--- B·∫Øt ƒë·∫ßu c√†o {len(links)} b√†i b√°o ---")

    for url in links:
        try:
            print(f"üîÑ ƒêang t·∫£i: {url} ...")
            article = Article(url, config=art_config)
            article.download()
            article.parse()
            
            text_content = article.text
            
            if len(text_content) < config.MIN_ARTICLE_LENGTH:
                print(f"‚ö†Ô∏è C·∫£nh b√°o: N·ªôi dung qu√° ng·∫Øn. B·ªè qua.")
                continue

            scraped_data.append({
                "url": url,
                "title": article.title,
                "text": text_content,
                "publish_date": str(article.publish_date)
            })
            print(f"‚úÖ ƒê√£ l·∫•y xong: {article.title} ({len(text_content)} k√Ω t·ª±)")
            
            time.sleep(config.SCRAPE_SLEEP) 

        except Exception as e:
            print(f"‚ùå L·ªói khi c√†o link {url}: {e}")

    return scraped_data

# Test Block
if __name__ == "__main__":
    results = scrape_content(config.STEP2_FILE)
    
    if results:
        with open(config.STEP3_FILE, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\n‚úÖ Node 3 Th√†nh c√¥ng! L∆∞u t·∫°i {config.STEP3_FILE}")
    else:
        print("\n‚ùå Kh√¥ng l·∫•y ƒë∆∞·ª£c b√†i n√†o ho·∫∑c file input r·ªóng.")